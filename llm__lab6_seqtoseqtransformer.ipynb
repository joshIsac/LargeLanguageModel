{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcaOx58Ea9MoGUjHOmyjKp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshIsac/LargeLanguageModel/blob/main/llm__lab6_seqtoseqtransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing library for Seq2seq transformer\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "collapsed": true,
        "id": "21bKu3Z0u_8_"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the datasets for language translation using datasets library\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "data = load_dataset(\"Helsinki-NLP/opus_books\", \"ca-en\")\n"
      ],
      "metadata": {
        "id": "1GXNPa4zwcAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize the data\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ca-en\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e7EJJgcyQNN",
        "outputId": "b17efca6-c400-42cc-d72f-dbd31f5e6707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize the data\n",
        "def tokenize_data(data):\n",
        "  input_ids =[]\n",
        "  attention_masks =[]\n",
        "  for item in data:\n",
        "    src_txt = item['translation']['ca']\n",
        "    tgt_txt = item['translation']['en']\n",
        "    encoded_dict = tokenizer(src_txt,return_tensors='pt',padding='max_length',max_length=128,truncation=True)\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "  return np.array(input_ids),np.array(attention_masks)\n",
        "input_ids,attention_masks = tokenize_data(data['train'])"
      ],
      "metadata": {
        "id": "t1iUP8D0HB-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztx7TZht2jGY",
        "outputId": "57699ccc-e55e-46f5-a6b0-c23f62a51604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 4571,   164,  1633, ..., 55254, 55254, 55254]],\n",
              "\n",
              "       [[  317,    77,   837, ..., 55254, 55254, 55254]],\n",
              "\n",
              "       [[ 3653,   301,  2901, ..., 55254, 55254, 55254]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[  394,  9596,  7353, ..., 55254, 55254, 55254]],\n",
              "\n",
              "       [[   86,  1569,   241, ..., 55254, 55254, 55254]],\n",
              "\n",
              "       [[11758,    85,  4210, ..., 55254, 55254, 55254]]])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_masks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4DGEC1i_b8W",
        "outputId": "55bab614-f417-4c41-c6ae-3df61ca06f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1, 1, 1, ..., 0, 0, 0]],\n",
              "\n",
              "       [[1, 1, 1, ..., 0, 0, 0]],\n",
              "\n",
              "       [[1, 1, 1, ..., 0, 0, 0]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[1, 1, 1, ..., 0, 0, 0]],\n",
              "\n",
              "       [[1, 1, 1, ..., 0, 0, 0]],\n",
              "\n",
              "       [[1, 1, 1, ..., 0, 0, 0]]])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "model=TFAutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-ca-en\")\n",
        "# Define data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQyns2_kFqsb",
        "outputId": "55cc0634-d849-492f-9473-0b630cab2ac4"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
            "\n",
            "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-ca-en.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare the dataset\n",
        "train_dataset=tf.data.Dataset.from_tensor_slices((input_ids,attention_masks)).batch(32)"
      ],
      "metadata": {
        "id": "ZFpIWfTtHPr6"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss and compile the model\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
      ],
      "metadata": {
        "id": "hkYP8ByII9G7"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_dataset,epochs=10)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dLfYg7mGJM7V"
      },
      "execution_count": 92,
      "outputs": []
    }
  ]
}